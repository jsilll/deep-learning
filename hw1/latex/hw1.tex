\documentclass[12pt]{article}

\usepackage{url}
\usepackage{color}
\usepackage{float}
\usepackage{array}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{hyperref}
\usepackage{spreadtab}
\usepackage{longtable}
\usepackage{pdflscape}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[margin=1.5cm]{geometry}
\usepackage[format=hang,labelfont=bf,font=small]{caption}
\usepackage{mathtools, amsmath, amsthm, amssymb, amsfonts, mathpartir}

\begin{document}

\title{Deep Learning: Homework 1}
\author{Hugo Mantinhas 95592, Jo√£o Silveira 95597}

\maketitle

\section*{Work Division}

Each member of the group worked together on all the questions in this assignment. For this reason, the workload was divided equally between the two members of the group.

\section*{Question 1}
\begin{itemize}
    \item \textbf{1. a)}
          \begin{table}[H]
              \centering
              \begin{tabular}{|c|c|c|c|}
                  \toprule
                                    & \textbf{Train} & \textbf{Validation} & \textbf{Test} \\
                  \midrule
                  \textbf{Accuracy} & 0.4654         & 0.4610              & 0.3422        \\
                  \bottomrule
              \end{tabular}
              \label{tab:1.1a}
              \caption{Train, validation and test accuracies for the perceptron model}
          \end{table}

          \begin{figure}[H]
              \centering
              \includegraphics[width=1\linewidth]{../outputs/hw1-q1-1a.png}
              \caption{Train and validation accuracies as a function of the number of epochs for the perceptron model}
              \label{fig:1.1a}
          \end{figure}
          \clearpage

    \item \textbf{1. b)} Analyzing the plot for the learning rate of 0.01, we find that the weight update many times results in accuracy changes in the order of 5\%, in both directions. This suggests that the weight adjustments might be overshooting the optimal value. On the other hand, the learning rate of 0.001 results in a more stable learning process, with the accuracy changes being in the order of 1\% or less. Unlike the model trained with 0.01 learning rate, this one seems to be tending to grow over time which suggests that the weight adjustments are closer to the optimal value. Although the final test accuracies for both models don't look much different: 0.5784 for learning rate 0.01 and 0.5936 for learning rate 0.001; for the latter the model shows a lot more stability in its accuracy across epochs.
          \begin{figure}[H]
              \centering
              \includegraphics[width=1\linewidth]{../outputs/hw1-q1-1b.01.png}
              \caption{Train and validation accuracies as a function of the number of epochs for learning rate of 0.01}
              \label{fig:1.1b:0.01}
          \end{figure}

          \begin{figure}[H]
              \centering
              \includegraphics[width=1\linewidth]{../outputs/hw1-q1-1b.001.png}
              \caption{Train and validation accuracies as a function of the number of epochs for learning rate of 0.001}
              \label{fig:1.1b:0.001}
          \end{figure}

          \begin{table}[H]
              \centering
              \begin{tabular}{|c|c|c|c|}
                  \toprule
                                               & \textbf{Train} & \textbf{Validation} & \textbf{Test} \\
                  \midrule
                  \textbf{Learning rate 0.01}  & 0.6609         & 0.6568              & 0.5784        \\
                  \textbf{Learning rate 0.001} & 0.6625         & 0.6639              & 0.5936        \\
                  \bottomrule
              \end{tabular}
              \label{tab:1.1b}
              \caption{Train, validation and test accuracies for learning rates of 0.01 and 0.001}
          \end{table}

    \item \textbf{2. a)} This statement is true. A logistic regression model is a linear model, which means that it can only learn linearly separable data, i.e., a dataset whose classes can be separated by a hyperplane. On the other hand, a multi-layer perceptron using \texttt{relu} activations can learn to separate non-linearly separable data because of the non-linearity introduced by the \texttt{relu} activation function in between layers. However, it can be shown, by computing the Hessian matrix of the loss function, that the loss function of a logistic regression model using cross-entropy loss is always convex. However, the same cannot be done for the loss function of a multi-layer perceptron, in general. This means that the logistic regression model can be trained to a global optimum, while, when using a multi-layer perceptron, we can never be sure that we have reached a global optimum.

    \item \textbf{2. b)} The final test accuracy is 0.7580.
          \begin{figure}[H]
              \centering
              \includegraphics[width=1\linewidth]{../outputs/hw1-q1-2b.001-acc.png}
              \caption{Train and validation accuracies as a function of the number of epochs for the no-toolkit MLP}
              \label{fig:1.2b:acc}
          \end{figure}

          \begin{figure}[H]
              \centering
              \includegraphics[width=1\linewidth]{../outputs/hw1-q1-2b.001-loss.png}
              \caption{Train loss as a function of the number of epochs for the no-toolkit MLP}
              \label{fig:1.2b:loss}
          \end{figure}
\end{itemize}

\section*{Question 2}
\begin{itemize}
    \item \textbf{1. )} The best configuration in terms of validation accuracy was the one with learning rate 0.01, with a validation accuracy of 0.6535. Its final test accuracy is 0.6200.
          \begin{figure}[H]
              \centering
              \includegraphics[width=0.5\linewidth]{../outputs/hw1-q2-1-acc.01.png}
              \caption{Validation accuracy as a function of the number of epochs for the logistic regression model with learning rate 0.01}
              \label{fig:2.1:acc}
          \end{figure}

          \begin{figure}[H]
              \centering
              \includegraphics[width=0.5\linewidth]{../outputs/hw1-q2-1-loss.01.png}
              \caption{Train and validation loss as function of the number of epochs for the logistic regression model with learning rate 0.01}
              \label{fig:2.1:loss}
          \end{figure}

    \item \textbf{2. a)}
          % Compare the performance of your model with batch sizes 16 and 1024 with the remaining hyperparameters at their default value.

          The choice of the batch size hyperparameter imposes dealing with a trade-off between accuracy and training time. Smaller batches produce more frequent gradient updates, but this comes at the cost of making more non-parallelizable computations. Moreover, as we decrease the batch size, the more noisy the gradient signal becomes, which, in the extreme case, can cause unstable training. However, when chosen properly, having a smaller batch size can also offer a regularizing effect, since it introduces noise in the gradient signal, which can help the model generalize better. Larger batch sizes, on the other hand, produce less frequent gradient updates, meaning that, in each epoch, we need to do less non-parallelizable computations and, therefore, each epoch takes less time to complete. However, in the extreme case, large batch sizes can lead to overfitting, since the gradient signal is less noisy. 

          In this case,

          \begin{table}[H]
              \centering
              \begin{tabular}{|c|cc|}
                  \toprule
                  \textbf{Batch Size} & \textbf{Test Accuracy} & \textbf{Execution time} \\
                  \midrule
                  \textbf{16}         & \underline{0.7788}     & 1min17.420sec           \\
                  \textbf{1024}       & 0.7208                 & 0min22.466sec           \\
                  \bottomrule
              \end{tabular}
              \caption{Test accuracy and execution time for the feed-forward network with batch sizes 16 and 1024}
              \label{tab:2.2a}
          \end{table}

          \begin{figure}[H]
              \centering
              \includegraphics[width=0.5\linewidth]{../outputs/hw1-q2-2a-acc-16.png}
              \caption{Train and validation accuracies as a function of the number of epochs for the feed-forward network using a batch size of 16}
              \label{fig:2.2a:acc:16}
          \end{figure}

          \begin{figure}[H]
              \centering
              \includegraphics[width=0.5\linewidth]{../outputs/hw1-q2-2a-loss-16.png}
              \caption{Train and validation loss as a function of the number of epochs for the feed-forward network using a batch size of 16}
              \label{fig:2.2a:loss:16}
          \end{figure}

          \begin{figure}[H]
              \centering
              \includegraphics[width=0.5\linewidth]{../outputs/hw1-q2-2a-acc-1024.png}
              \caption{Validation accuracy as a function of the number of epochs for the feed-forward using a batch size of 1024}
              \label{fig:2.2a:acc:1024}
          \end{figure}

          \begin{figure}[H]
              \centering
              \includegraphics[width=0.5\linewidth]{../outputs/hw1-q2-2a-loss-1024.png}
              \caption{Train and validation loss as a function of the number of epochs for the feed-forward using a batch size of 1024}
              \label{fig:2.2a:loss:1024}
          \end{figure}
\end{itemize}

\section*{Question 3}
\begin{itemize}
    \item \textbf{1. a)} For the case with $D=2, A=-1, B=1$, there are 4 data points in our domain, calculated in Figure \ref{fig:3a:calc}, and represented in Figure \ref{fig:3a:graph}

          \begin{figure}[H]
              \begin{minipage}{0.5\linewidth}
                  \begin{equation*}
                      f(-1, 1):
                  \end{equation*}
                  \begin{equation*}
                      \sum_{i=1}^{2} x_i = -1 + 1 = 0
                  \end{equation*}
                  \begin{equation*}
                      0 \in [-1, 1] \Rightarrow f(-1, 1) = 1
                  \end{equation*}

                  \begin{equation*}
                      f(-1, -1):
                  \end{equation*}
                  \begin{equation*}
                      \sum_{i=1}^{2} x_i = -1 + -1 = -2
                  \end{equation*}
                  \begin{equation*}
                      -2 \notin [-1, 1] \Rightarrow f(-1, -1) = -1
                  \end{equation*}
              \end{minipage}
              \begin{minipage}{0.5\linewidth}
                  \begin{equation*}
                      f(1, 1):
                  \end{equation*}
                  \begin{equation*}
                      \sum_{i=1}^{2} x_i = 1 + 1 = 2
                  \end{equation*}
                  \begin{equation*}
                      2 \notin [-1, 1] \Rightarrow f(1, 1) = -1
                  \end{equation*}

                  \begin{equation*}
                      f(1, -1):
                  \end{equation*}
                  \begin{equation*}
                      \sum_{i=1}^{2} x_i = 1 + (-1) = 0
                  \end{equation*}
                  \begin{equation*}
                      0 \in [-1, 1] \Rightarrow f(1, -1) = 1
                  \end{equation*}
              \end{minipage}
              \caption{Calculation of $f(x)$ for $D=2, A=-1, B=1$}
              \label{fig:3a:calc}
          \end{figure}

          \begin{figure}[H]
              \centering
              \includegraphics[width=0.5\linewidth]{../outputs/hw1-q3-a.png}
              \caption{Space of $f(x)$ for $D=2, A=-1, B=1$. Green represents class 1 and red represents class -1.}
              \label{fig:3a:graph}
          \end{figure}

          For this example, we can see that the data is not linearly separable, since it is equivalent to the XOR problem which is also not linearly separable. For this reason, we can conclude we cannot use a single perceptron to classify this data.

\end{itemize}

\end{document}
